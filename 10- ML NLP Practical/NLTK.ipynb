{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fcc93b",
   "metadata": {},
   "source": [
    "## Getiing Familiar with nltk library\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0308c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f2a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ddfab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At eight o'clock on Thursday morning\\nArthur didn't feel very good.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107ac54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9535200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rakesh_PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rakesh_PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rakesh_PC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7aa07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d9161",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ea9a1",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenizing\n",
    "\n",
    "**By tokenizing, you can conveniently split up text by word or by sentence.** This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. It’s your first step in turning unstructured data into structured data, which is easier to analyze.\n",
    "\n",
    "When you’re analyzing text, you’ll be tokenizing by word and tokenizing by sentence. Here’s what both types of tokenization bring to the table:\n",
    "\n",
    "#### Tokenizing by word: \n",
    "Words are like the atoms of natural language. They’re the smallest unit of meaning that still makes sense on its own. Tokenizing your text by word allows you to identify words that come up particularly often. For example, if you were analyzing a group of job ads, then you might find that the word “Python” comes up often. That could suggest high demand for Python knowledge, but you’d need to look deeper to know more.\n",
    "\n",
    "#### Tokenizing by sentence: \n",
    "\n",
    "When you tokenize by sentence, you can analyze how those words relate to one another and see more context. Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?\n",
    "\n",
    "Here’s how to import the relevant parts of NLTK so you can tokenize by word and by sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae2682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8addc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "... And the first lesson of all was the basic trust that he could learn.\n",
    "... It's shocking to find how many people do not believe they can learn,\n",
    "... and how many more believe learning to be difficult.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98de822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Muad'Dib learned rapidly because his first training was in how to learn.\\nAnd the first lesson of all was the basic trust that he could learn.\\nIt's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24a070ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muad'Dib learned rapidly because his first training was in how to learn.\n",
      "And the first lesson of all was the basic trust that he could learn.\n",
      "It's shocking to find how many people do not believe they can learn,\n",
      "and how many more believe learning to be difficult.\n"
     ]
    }
   ],
   "source": [
    "print(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea2091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use sent_tokenize() to split up example_string into sentences:\n",
    "sentences = sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fe9164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1b0432e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd2d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try tokenizing example_string by word:\n",
    "words = word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5440f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Muad'Dib\",\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e246ee4",
   "metadata": {},
   "source": [
    "You got a list of strings that NLTK considers to be words, such as:\n",
    "\n",
    "- \"Muad'Dib\"\n",
    "- 'training'\n",
    "- 'how'\n",
    "\n",
    "But the following strings were also considered to be words:\n",
    "\n",
    "- \"'s\"\n",
    "- ','\n",
    "- '.'\n",
    "\n",
    "See how \"It's\" was split at the apostrophe to give you 'It' and \"'s\", but \"Muad'Dib\" was left whole? This happened because NLTK knows that 'It' and \"'s\" (a contraction of “is”) are two distinct words, so it counted them separately. But \"Muad'Dib\" isn’t an accepted contraction like \"It's\", so it wasn’t read as two separate words and was left intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b9715",
   "metadata": {},
   "source": [
    "#### Filtering Stop Words\n",
    "**Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it**. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a007ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rakesh_PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f788327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab2af82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60699a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_quote = word_tokenize(worf_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88bef36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7feb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of stop words to filter words_in_quote. For this example, you’ll need to focus on stop words in \"english\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74a7602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa12b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []\n",
    "\n",
    "for i in words_in_quote:\n",
    "    if i not in stop_words:\n",
    "        filtered_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2264125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73469e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = []\n",
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4b023",
   "metadata": {},
   "source": [
    "You used `.casefold()` on word so you could ignore whether the letters in word were `uppercase or lowercase`. This is worth doing because stopwords.words('english') includes only lowercase versions of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "036ddeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> filtered_list = [\n",
    "# ...     word for word in words_in_quote if word.casefold() not in stop_words\n",
    "# ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "375fbab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7661f41",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. NLTK has more than one stemmer, but you’ll be using the `Porter stemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08bf5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e5d8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create a stemmer with PorterStemmer():\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cb1d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_stemming = \"\"\"\n",
    "... The crew of the USS Discovery discovered many discoveries.\n",
    "... Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c0f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "934404d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'USS',\n",
       " 'Discovery',\n",
       " 'discovered',\n",
       " 'many',\n",
       " 'discoveries',\n",
       " '.',\n",
       " 'Discovering',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explorers',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98cf8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the stemmed versions of the words in words by using \n",
    "# stemmer.stem() in a list comprehension:\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e80ebd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fa272",
   "metadata": {},
   "source": [
    "Those results look a little inconsistent. Why would 'Discovery' give you 'discoveri' when 'Discovering' gives you 'discov'?\n",
    "\n",
    "Understemming and overstemming are two ways stemming can go wrong:\n",
    "\n",
    "- **Understemming** happens when two related words should be reduced to the same stem but aren’t. This is a false negative.\n",
    "- **Overstemming** happens when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc117e3d",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "Now that you’re up to speed on parts of speech, you can circle back to lemmatizing. Like stemming, **lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'.**\n",
    "\n",
    "Note: **A lemma is a word that represents a whole group of words, and that group of words is called a lexeme.**\n",
    "\n",
    "For example, if you were to look up the word “blending” in a dictionary, then you’d need to look at the entry for “blend,” but you would find “blending” listed in that entry.\n",
    "\n",
    "In this example, “blend” is the lemma, and “blending” is part of the lexeme. So when you lemmatize a word, you are reducing it to its lemma.\n",
    "\n",
    "Here’s how to import the relevant parts of NLTK in order to start lemmatizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e412ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "078dff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7546517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"scarves\")\n",
    "# 'scarf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bfaed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03df7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dae416e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e76568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b31ab37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3dd7ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worst'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\")\n",
    "# 'worst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb17379f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")\n",
    "# 'bad'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff7699",
   "metadata": {},
   "source": [
    "You got the result 'worst' because lemmatizer.lemmatize() assumed that \"worst\" was a noun. You can make it clear that you want \"worst\" to be an adjective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd0bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b51d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
